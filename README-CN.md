# 基于 HMM 的词性标注

Python 实现的简单词性标注，基于`隐马尔科夫模型`。

本项目的贡献：

- 一个词性标注的简单解决方案。
- 对 HMM 的实现与应用。
- 炫酷的自制文本预处理工具。

## 简介

### 工作流程

- **9 月 20 日** 获取语料，完成数据清洗。
- **9 月 21 日** 实现有关矩阵的 3 个主要函数，包括“读”、“存”、“归一化”。
- **9 月 22 日** 使用多线程加速训练。对项目架构进行微小改动。
- **9 月 23 日** 实现预测方法。
- **9 月 30 日** 搭建测试函数框架。重构语料架构。
- **10 月 1 日** 完成主函数。对项目架构进行微小变动。
- **10 月 12 日** 对项目进行微调。引入超参数。
- **10 月 15 日** 反复试验，努力提高测试的准确率。

### 项目亮点

- 在中等规模的数据集(2329263 行，531 MB)上训练，并取得良好效果（准确率 92.69%）。
- 实现并行计算，训练速度提升大约 40%（训练 25s，测试 13s）。
- 对模型预测的错误结果进行了细致的分析来优化模型。

### 使用样例

用于输入的使用样例被存放在 “example.txt” 文件中。

example.txt 提供了 10 条中等长度的句子。

你可以用如下命令使用样例输入：

```bash
python main.py < example.txt
```

输入后，你将能够看到程序是如何工作的。

## 语料

### 概览

语料库是中文的。它们可以在 [liwenzhu 的代码仓库](https://github.com/liwenzhu/corpusZh) 找到。感谢这位仁兄的辛勤工作！

该语料库包含了 35 种词性，总计 2329263 行，531 MB。

| 代码 |   意义   | 代码 |   意义   | 代码 |   意义   | 代码 |     意义     | 代码 |      意义      |
| :--: | :------: | :--: | :------: | :--: | :------: | :--: | :----------: | :--: | :------------: |
|  n   | 普通名词 |  nt  | 时间名词 |  nd  | 方位名词 |  nl  |   处所名词   |  nh  |      人名      |
| nhf  |    姓    | nhs  |    名    |  ns  |   地名   |  nn  |     族名     |  ni  |     机构名     |
|  nz  | 其他专名 |  v   |   动词   |  vd  | 趋向动词 |  vl  |   联系动词   |  vu  |    能愿动词    |
|  a   |  形容词  |  f   |  区别词  |  m   |   数词   |  q   |     量词     |  d   |      副词      |
|  r   |   代词   |  p   |   介词   |  c   |   连词   |  u   |     助词     |  e   |      叹词      |
|  o   |  拟声词  |  i   |  习用语  |  j   |  缩略语  |  h   |   前接成分   |  k   |    后接成分    |
|  g   |  语素字  |  x   | 非语素字 |  w   | 标点符号 |  ws  | 非汉字字符串 |  wu  | 其他未知的符号 |

### 处理

原始语料库已经标注并有较高质量，但为了项目需要仍然做了简单的清理任务，主要是删除空格。

```bash
sed -i 's/[][]//g' origin.txt             # Delete the ']' and '['.
sed -i -E 's/ +/ /g' origin.txt           # Delete repeated spaces.
sed -i 's/\s*\//\//g' origin.txt          # Delete spaces before the '/'.
sed -i -E 's/([^a-z]) +/\1/g' origin.txt  # Delete spaces after the non-alpha char.
```

## 数据集

### 切割

考虑到 GitHub 对单文件大小的限制和内存读取速度，决定将原始语料文件划分成多个小文件。因此……

```bash
split -l 200000 all.txt                   # Split large file to small files.
```

分割后的文件全部存储在文件夹 `./corpus/train` 并按从 “xaa” 到 “xal” 命名。

除了 “xal” 有 129263 行，其余文件都是 200000 行并且平均文件大小为 45 MB。

相对地，测试集被存储在 `./corpus/test` 并按从 “yaa” 到 “yak” 命名。

所有的测试集都是利用 shell 命令 `shuf` 从不参与训练的集合 `train/xal` 随机生成的。

```bash
shuf -n200 train/yal > test/yaa           # Generate the test data.
```

### 划分

关于数据集的划分，主要考虑到训练过程和测试过程的时间复杂度。

对于每个长度为 $T$ 的文本，假设存在 $N$ 中隐藏状态。

- 训练过程几乎只从头到尾遍历句子一遍，因此时间复杂度是 $O(T)$。
- 然而测试使用 Viterbi 算法，需要计算每次隐藏状态之间的乘积，因此花费 $O(T \times N \times N)$。

测试时间是训练时间的 $N^2$ 倍，因此数据集应该按照这个比例进行划分。

在我们的例子中，$N$ 等于 35，因此测试集与训练集的比率应当接近 $1:1225$。

### 元数据

| train set |   file size    | test set |  file size  |
| :-------: | :------------: | :------: | :---------: |
| train/xaa | 200000L, 45.2M | test/yaa | 200L, 56.0K |
| train/xab | 200000L, 47.0M | test/yab | 200L, 57.8K |
| train/xac | 200000L, 47.5M | test/yac | 200L, 34.6K |
| train/xad | 200000L, 47.1M | test/yad | 200L, 36.9K |
| train/xae | 200000L, 47.3M | test/yae | 200L, 37.6K |
| train/xaf | 200000L, 44.9M | test/yaf | 200L, 43.3K |
| train/xag | 200000L, 43.6M | test/yag | 200L, 52.9K |
| train/xah | 200000L, 45.2M | test/yah | 200L, 60.0K |
| train/xai | 200000L, 45.1M | test/yai | 200L, 45.7K |
| train/xaj | 200000L, 43.9M | test/yaj | 200L, 53.2K |
| train/xak | 200000L, 43.7M | test/yak | 200L, 52.8K |

`common.dataset` 模块提供了引用数据集的简单方法。

## 模型

### 矩阵

所有矩阵都从原始语料库中读取，并保存为 json 文件。

在内存中，它们存储在 python-dict 中，可以用 $O(1)$ 读取。

读取和保存 3 矩阵的方法都在模块 `model.matrix` 中实现。

这里描述了 3 矩阵的元数据。

```python
init_mt = {state:prob * 35}
tran_mt = {prev:{curr:prob * 35} * 35}
emis_mt = {state:{obs:prob * N} * 35}
```

从原始语料库中提取的三个矩阵的大小为：

初始矩阵912 B；转换矩阵28.9K；发射矩阵7.64M；

### 算法

该模型以 `隐马尔可夫模型` 为基础，采用 `Viterbi` 算法进行计算。

有关该算法的详细信息，请访问 [这里](https://en.wikipedia.org/wiki/Viterbi_algorithm)。

该程序的模型是用 `model.markov` 和 `model.witerbi` 模块实现的。

前者实现了马尔可夫链的生成和回溯，后者实现了对马尔可夫链的更新。

## 调试

### 问题一：0 值

##### 问题描述

在调试过程中，主要遇到 `0 值问题`。0 值问题由以下几个方面引起：

- 由于语料库的稀疏性，转换概率和发射概率的值非常小。
- 马尔可夫需要前后概率的累乘，因此概率在这段时间内持续缩小。
- 当序列太长时，概率可能低于计算机所能表示的最小正数。

除了 **计算机表示能力** 的限制外，0 值问题还表现为 **对可用信息的浪费**。

- 转移矩阵和稀疏矩阵实际上是包含大量 0 值的稀疏矩阵。
- 因此，该模型在计算概率时会遇到乘以 0 的情况。
- 这将导致任何后续操作的结果都为 0，从而浪费可用信息。

##### 解决方案

为了解决0值问题，所采用的解决方案是用一个非常小的数字替换可能的 0 值。

在实际项目中，使用了“1e-99”。

具体而言，当遇到不在语料库中的单词或前一节点的所有状态的最大概率为零时，将概率设置为1e-99，而不是0。

### 问题二：mq 标记

#### 问题描述

在实际测试中，还发现了原始数据集的问题。那就是标签 “mq”，它没有出现在元数据描述中。

- 在原始数据集中，“m” 表示数字，“q” 表示量词，不存在 “mq”。
- 经过调查，“mq” 主要标记了 **“一个“** 和 **”几个“** 这样的词语。
- “mq” 的存在严重降低了模型预测的准确性。

##### 解决方案

在训练和测试时，将“mq”替换为“m”。

```python
if state == "mq":
    state = "m"
```

这一改动对原始模型架构的影响很小，但模型准确性的提高是巨大的。

添加 “mq” 后，模型的准确性提高了约0.44%。

## 试验

### 可调参数

模型的性能与超参数的选择密切相关。

所有超参数都可以在 `common.params` 模块中找到。

本项目共定义了三个超参数：

```python
MINIMAL_PROB = 1e-99
THRESHOULD = 1e-3
ENDSTATE_GUESS = "w"
```

- **MINIMAL_PROB**：
用于替换概率 0 的最小概率，用于解决 0 值问题。

- **THRESHOULD**：
如果某个结束状态的概率大于状态 “w” 的概率和阈值之和，就使用该状态作为最终状态。

- **ENDSTATE_GUESS**：
默认情况下，猜测最后一个节点的状态是 “w”（标点符号）。

-**MIDSTATE_GUESS**：
如果前一个节点的所有状态的最大概率为0，则猜测前一个结点的隐藏状态为 “n”（名词）。

```python
# find the end state with the highest probability

# by default guess the 'w'
end_state = ENDSTATE_GUESS
end_prob = markov_chain[-1][ENDSTATE_GUESS]["prob"]

for state in markov_chain[-1]:
    prob = markov_chain[-1][state]["prob"]
    # update if beyond the threshould
    if prob > end_prob + THRESHOULD:
        end_prob = prob
        end_state = state
```

### 试验结果

##### 最终结果

```
Last tested on 2023-10-18 00:40:40.062570.
Total number of test samples is 69290
There are 64225 correctly tagged and 5065 incorrectly.
Accuracy is 92.69%
Error prediction distribution:
n:1040	nt:69	nd:81	nl:33	nh:14	nhf:12	nhs:7	ns:74
nn:0	ni:14	nz:3	v:1122	vd:89	vl:115	vu:31	a:354
f:15	m:84	q:62	d:410	r:103	p:183	c:121	u:499
e:0	o:1	i:95	j:48	h:11	k:30	g:0	x:1
w:302	ws:42	wu:0	
```

本项目对模型的优化主要基于对错误预测样本的分析。

`优化 1`

在第一轮试验中，模型的准确率达到了 83.77%，这是比较高的准确率了。当我深入研究错误样本时，发现模型对于标记 “w”（标点符号）最容易分错，出错次数达到了 2000 以上。

经测试，这是由于测试句子较长，最终决策节点各个状态概率相差较小，模型对于本来应该位于句子末尾的标点符号 “w” 标记偏好较低。

为了解决这个问题，引入了超参数 **ENDSTATE_GUESS** 和 **THRESHOULD** 来为模型添加偏好，使之默认猜测 “w” 从而提高了准确率。

`优化 2`

在第二轮试验中，注意到模型对于 “n”（名词）和 “v”（动词）容易分错，分错次数都达到了 1000 以上。

这是语料库本身的不完备导致的。即使是中等规模的语料集也难以覆盖全部的语言词汇，因此模型对这些词汇感到 “陌生”。

为了解决这个问题，引入了超参数 **MIDSTATE_GUESS** 来猜测在执行维特比算法时遇到的不认识的单词，使之默认猜测 “n” 从而提高了准确率。

经测试，令之默认猜测 “v” 也能达到类似的效果提升，处于一般性的考虑，将其设为 “n”。

`其他`

经测试，将 **MINIMAL_PROBABILITY** 调得更小将会使训练的准确率提升更高，但会带来过拟合的问题。

特别是句子长度特别长的情况下，模型最后会将所有词语都预测为 “n”，也就是预设的 **MIDSTATE_GUESS**。

综合考虑以后，仍然选择较大的 1e-99 作为 **MINIMAL_PROBABILITY**。
